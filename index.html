<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <title>Zifeng Wang's Homepage</title>

  <meta name="author" content="Zifeng Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="js/jquery.min.js"></script>
  <script src="js/jquery.scrollzer.min.js"></script>
  <script src="js/jquery.scrolly.min.js"></script>
  <script src="js/skel.min.js"></script>
  <script src="js/skel-layers.min.js"></script>

  <script type="text/javascript">
    function toggle_vis(id) {
      var e = document.getElementById(id);
      if (e.style.display == 'none')
        e.style.display = 'inline';
      else
        e.style.display = 'none';
    }
  </script>

  <!-- <script src="js/init.js"></script> -->
  <script src="js/carousel.js"></script>

  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="icon" type="image/png" href="images/zifeng_avatar.jpg">


  <style type="text/css">
    .carousel {
      -webkit-transform: translate3d(0, 0, 0);
      background: rgba(0, 0, 0, 0.85);
      position: fixed;
      right: 0;
      bottom: 0;
      min-width: 100%;
      min-height: 100%;
      width: auto;
      height: auto;
      display: none;
      z-index: 1;
    }

    .img-center-carousel {
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      padding: 0;
      margin: auto;
      width: 60%;
      height: auto;
    }

    .carousel-close {
      position: absolute;
      top: 25px;
      right: 100px;
      padding: 0;
      margin: auto;
      width: 50px;
      height: auto;
    }

    .research {
      margin-bottom: 30px;
    }

    .research h4 {
      float: left;
    }

    .research div {
      text-align: end;
      font-size: 0.9em;
    }

    .thumbnail {
      width: 100%;
      float: left;

    }

    .thumbnail-right {
      margin-left: 35%;
    }

    #exp li {
      margin-bottom: 50px;
    }

    .school-logo {
      width: 6%;
      float: left;
    }

    .school-text {
      margin-left: 10%;
      width: 60%;
    }
  </style>

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zifeng Wang</name>
                  </p>
                  <p> I am a research scientist at Google. I received my PhD in Machine Learning at <a
                      href="https://web.northeastern.edu/spiral/">SPIRAL Group</a>
                    from <a href="https://www.northeastern.edu/">Northeastern University</a>, advised by
                    Prof. <a href="http://www.ece.neu.edu/fac-ece/jdy/">Jennifer G. Dy</a>.
                    During my PhD, I also work closely with Prof. <a href="https://ece.northeastern.edu/fac-ece/ioannidis/">Stratis
                      Ioannidis</a>
                    and Prof. <a
                      href="https://web.northeastern.edu/yanzhiwang/#_ga=2.65760046.1809272488.1657261940-1545783568.1564022696">Yanzhi
                      Wang</a>.
                    <!-- I also work as student researcher at <a href="https://research.google/teams/cloud-ai/">Google Cloud
                      AI Research</a>,
                    under the supervision of <a href="https://sites.google.com/view/zizhaozhang/home">Zizhao Zhang</a>,
                    <a href="https://chl260.github.io/">Chen-Yu Lee</a>, and <a href="https://tomas.pfister.fi/">Tomas
                      Pfister</a>.
                    My other collaborators at Google includes <a href="https://people.eecs.berkeley.edu/~sayna/">Sayna
                      Ebrahimi</a>,
                    <a href="https://sites.google.com/view/hanzhang">Han Zhang</a>,
                    <a href="https://research.google/people/VincentPerot/">Vincent Perot</a>,
                    <a href="https://research.google/people/107194/">Ruoxi Sun</a>,
                    <a href="https://research.google/people/GuolongSu/">Guolong Su</a>,
                    <a href="https://resnick.caltech.edu/people/xiaoqi-ren">Xiaoqi Ren</a>,
                    <a href="https://www.linkedin.com/in/hao-zhang-4a7aab56/">Hao Zhang</a> and
                    <a href="https://research.google/people/106320/">Jacob Devlin</a>.
                  </p> -->
                  I received my BS degree in Electronic Engineering from <a
                      href="http://www.tsinghua.edu.cn/publish/thu2018en/index.html">Tsinghua University</a>.
                    In my college years, I also worked with Prof. <a
                      href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a> (Tsinghua), Prof. <a
                      href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a> (Princeton) on computer vision and
                    Prof. <a href="http://fi.ee.tsinghua.edu.cn/~liyong/">Yong Li</a> (Tsinghua) on big data.
                  <p>
                    <!-- <mark> -->
                      <b>
                      I am looking for self-motivated student researchers / research interns with interests and expertise in LLMs! Feel free contact me if you would like to work on cutting-edge challenges in machine learning and LLMs.
                      </b>
                    <!-- </mark> -->
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:zifengwangking@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.co.il/citations?user=N1uBekcAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/zifeng-wang-21b069b4/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://twitter.com/zifengwang315">Twitter</a> &nbsp/&nbsp
                    <a href="ZifengWang-Resume.pdf">Resume</a> &nbsp/&nbsp
                    <a href="ZifengWang-CV.pdf">CV</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:20%;max-width:20%">
                  <a href="images/zifeng_avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zifeng_avatar.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                  <ul>
                    <li> Large language model (LLM) Alignment</li>
                    <li> Continual (Lifelong) learning </li>
                    <li> Open set recognition / novel class discovery </li>
                    <li> Adversarial robustness and model compression </li>
                    <li> Deep learning applications in computer vision, natural language understanding, document
                      understanding, healthcare, wireless communications, etc. </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                  <ul>
                    <li> 07/2023: I will be giving a talk on effective and efficient continual learning at <b> <a
                      href="https://www.a-star.edu.sg/cfar/events/effective-and-efficient-continual-learning">A*STAR</a> </b>, thanks for hosting!
                    <li> 05/2023: Our paper on document entity extraction is accepted at <b> ACL 2023 </b> (Findings)
                    <li> 04/2022: I received the <b> Outstanding Student Award in Research</b> from COE, Northeastern
                      University </li>
                    <li> 04/2023: Our paper on continual learning is accepted at <b> ICML 2023 </b>, code will be
                      released soon! </li>
                    <li> 02/2023: I am glad to give talks about efficient and sparse continual learning at <b> <a
                          href="https://www.continualai.org/">ContinualAI</a> </b> and <b> <a
                          href="https://www.aitime.cn/">AI Time</a> </b> </li>
                    <li> 10/2022: I received the <b> Scholar Award for NeurIPS 2022 </b> </li>
                    <li> 09/2022: Our paper on efficient continual learning is accepted at <b> NeurIPS 2022 </b> </li>
                    <li> 09/2022: I am glad to give a talk about prompting-based continual learning at <b> <a
                          href="https://www.continualai.org/">ContinualAI</a></b></li>
                    <li> 09/2022: Our paper on adversarially robust pruning is accepted at <b> ICDM 2022 </b> </li>
                    <li> 07/2022: Our paper on prompting-based continual learning is accepted at <b> ECCV 2022 </b>
                    </li>
                    <li> 03/2022: Our paper on prompting-based continual learning is accepted at <b> CVPR 2022 </b>
                    </li>
                    <li> <a href="javascript:toggle_vis('news')">Earlier news</a> </li>
                    <div id="news" style="display:none">
                      <li> 02/2022: Our paper on Bayesian continual learning is accepted by <b> Neural Networks </b>
                      </li>
                      <li> 09/2021: Our paper on HSIC for adversarial robustness is accepted at <b> NeurIPS 2021 </b>
                      </li>
                      <li> 09/2021: Our paper on deep learning for smoking prediction is accepted by <b> PLOS
                          Computational Biology </b> </li>
                      <li> 05/2021: I started my research internship at <b> <a
                            href="https://research.google/teams/cloud-ai/">Google Cloud AI Research</a> </b> </li>
                      <li> 09/2020: Our paper on feature grouping is accepted at <b> NeurIPS 2021 </b> </li>
                      <li> 08/2020: Two papers on continual learning and open-world class discovery are accepted at <b>
                          ICDM 2020 </b>, with one selected as <b>
                          <font color=red> the best paper candidate </font>
                        </b> </li>
                      <li> 08/2019: One paper on unseen class detection is accepted at <b> DySPAN 2019 </b> as the <b>
                          <font color=red> best paper </font>
                        </b> </li>
                    </div>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experiences</heading>
                  <p>
                  <ul id="exp" style="list-style:none;">
                    <li> <img class="school-logo" src="images/exp_logos/g.png">
                      <div class="school-text">Aug 2023 - Present, Google, <br /> Research Scientist
                        at <a href="https://research.google/teams/cloud-ai/">Cloud AI Research</a></div>
                    </li>
                    <li> <img class="school-logo" src="images/exp_logos/NEU.jpeg">
                      <div class="school-text">Sep 2018 - July 2023, Northeastern University, <br />Research assistant at
                        <a href="https://web.northeastern.edu/spiral/">SPIRAL Group</a>
                      </div>
                    </li>
                    <li> <img class="school-logo" src="images/exp_logos/g.png">
                      <div class="school-text">June 2021 - Jan 2023, Google, <br /> Student researcher / Research Intern
                        at <a href="https://research.google/teams/cloud-ai/">Cloud AI Research</a></div>
                    </li>
                    <li> <img class="school-logo" src="images/exp_logos/Tsinghua.jpeg">
                      <div class="school-text">Feb 2017 - July 2018, Tsinghua University, <br />Research assistant at <a
                          href="http://ivg.au.tsinghua.edu.cn/index.php">i-Vision Group</a> </div>
                    </li>
                    <li> <img class="school-logo" src="images/exp_logos/UMich.jpeg">
                      <div class="school-text">July 2017 - Sep 2017, University of Michigan, <br />Visiting researcher
                        at <a href="https://pvl.cs.princeton.edu/"> Vision & Learning Lab</a> </div>
                    </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Selected Publications</heading> <br>
                <a href="https://scholar.google.co.il/citations?user=N1uBekcAAAAJ&hl=en">Google Scholar</a>
                <font color="grey">for all publications</font>.
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/chain-of-table.png" data-id="qf-carousel"></img>
                <div id="qf-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/chain-of-table.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="qf-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2401.04398.pdf">
                  <papertitle>Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding
                  </papertitle>
                </a>
                <br>
                Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, <b>Zifeng Wang</b>, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister
                  <br>
                <em> International Conference on Learning Representations (<b>ICLR</b>), 2024. <br> </em>
                [<a href="https://arxiv.org/pdf/2401.04398.pdf">paper</a>]
                <br>
                <p></p>
                <p> CHAIN-OF-TABLE enhances the reasoning capability of LLMs by leveraging tabular structures to express intermediate thoughts for table-based reasoning. It instructs LLMs to dynamically plan an operation chain according to the input table and its associated question. </p>
              </td>
            </tr>


            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/queryform.png" data-id="qf-carousel"></img>
                <div id="qf-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/queryform.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="qf-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2211.07730.pdf">
                  <papertitle>QueryForm: A Simple Zero-shot Form Entity Query Framework</papertitle>
                </a>
                <br>
                <b>Zifeng Wang</b>, Zizhao Zhang, Jacob Devlin, Chen-Yu Lee, Guolong Su, Hao Zhang, Jennifer Dy, Vincent
                Perot, Tomas Pfister<br>
                <em> Findings of the Association for Computational Linguistics: <b>ACL</b> 2023 <br> </em>
                [<a href="https://arxiv.org/pdf/2211.07730.pdf">paper</a>]
                <br>
                <p></p>
                <p> QueryForm consists of a novel prompting-based framework for zero-shot document entity recognition
                  with large language models (LLMs), and
                  a large-scale weakly-supervised pre-training method on publicly available webpages. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/dualhsic.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/dualhsic.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2305.00380.pdf">
                  <papertitle>DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning</papertitle>
                </a>
                <br>
                <b>Zifeng Wang*</b>, Zheng Zhan*, Yifan Gong, Yucai Shao, Stratis Ioannidis, Yanzhi Wang, Jennifer
                Dy<br>
                <em>International Conference on Machine Learning (<b>ICML</b>), 2023. <br> </em>
                [<a href="https://arxiv.org/pdf/2305.00380.pdf">paper</a>][<a href="https://github.com/zhanzheng8585/DualHSIC">code</a>]
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> DualHSIC consists of two complementary components that stem from the Hilbert Schmidt independence
                  criterion (HSIC): HSIC-Bottleneck for Rehearsal (HBR) lessens the inter-task
                  interference and HSIC Alignment (HA) promotes task-invariant knowledge sharing. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/sparcl.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/sparcl.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2209.09476.pdf">
                  <papertitle>SparCL: Sparse Continual Learning on the Edge</papertitle>
                </a>
                <br>
                <b>Zifeng Wang*</b>, Zheng Zhan*, Yifan Gong, Geng Yuan, Wei Niu, Tong Jian,
                Bin Ren, Stratis Ioannidis, Yanzhi Wang, Jennifer Dy<br>
                <em>Neural Information Processing Systems (<b>NeurIPS</b>), 2022. <br> </em>
                [<a href="https://arxiv.org/pdf/2209.09476.pdf">paper</a>] [<a
                  href="https://github.com/neu-spiral/SparCL">code</a>]
                <!-- [<a href="">paper</a>] [<a
                  href="">code</a>] -->
                <br>
                <p></p>
                <p> SparCL explores sparsity for efficient continual learning and achieves both training acceleration
                  and accuracy preservation through the synergy of three aspects: weight sparsity,
                  data efficiency, and gradient sparsity. </p>
              </td>
            </tr>


            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/dualprompt.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/dualprompt.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2204.04799.pdf">
                  <papertitle>DualPrompt: Complementary Prompting for
                    Rehearsal-free Continual Learning</papertitle>
                </a>
                <br>
                <b>Zifeng Wang</b>, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong
                Su, Vincent Perot, Jennifer Dy, Tomas Pfister<br>
                <em>European Conference on Computer Vision (<b>ECCV</b>), 2022. <br> </em>

                [<a href="https://arxiv.org/pdf/2204.04799.pdf">paper</a>] [<a
                  href="https://github.com/google-research/l2p">code</a>]
                <br>
                <p></p>
                <p> DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone,
                  and then formulates the continual learning objective as learning task-invariant and task-specific
                  “instructions". </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/l2p.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/l2p.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2112.08654.pdf">
                  <papertitle>Learning to Prompt for Continual Learning</papertitle>
                </a>
                <br>
                <b>Zifeng Wang</b>, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent
                Perot, Jennifer Dy, Tomas Pfister<br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022. <br> </em>

                [<a href="https://arxiv.org/pdf/2112.08654.pdf">paper</a>] [<a
                  href="https://github.com/google-research/l2p">code</a>] [<a
                  href="https://ai.googleblog.com/2022/04/learning-to-prompt-for-continual.html">blog</a>]
                <br>
                <p></p>
                <p> We propose a new learning paradigm for continual learning: our method learns to dynamically
                  prompt (L2P) a pre-trained model to learn tasks sequentially under different task transitions. </p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/dbull.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/dbull.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2106.07035.pdf">
                  <papertitle>Deep Bayesian Unsupervised Lifelong Learning</papertitle>
                </a>
                <br>
                Tingting Zhao*, <b>Zifeng Wang*</b>, Aria Masoomi, Jennifer Dy<br>
                <em>Neural Networks, 2022. <br> </em>

                [<a href="https://arxiv.org/pdf/2106.07035.pdf">paper</a>] [<a
                  href="https://github.com/KingSpencer/DBULL">code</a>]
                <br>
                <p></p>
                <p> We develop a fully Bayesian inference framework for ULL with a novel end-to-end Deep Bayesian
                  Unsupervised Lifelong
                  Learning (DBULL) algorithm. </p>
              </td>
            </tr>



            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/hbar.png" data-id="hbar-carousel"></img>
                <div id="hbar-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/hbar.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="hbar-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2106.02734.pdf">
                  <papertitle>Revisiting Hilbert-Schmidt Information Bottleneck
                    for Adversarial Robustness</papertitle>
                </a>
                <br>
                <b>Zifeng Wang*</b>, Tong Jian*, Aria Masoomi, Stratis Ioannidis, Jennifer Dy<br>
                <em>Neural Information Processing Systems (<b>NeurIPS</b>), 2021. <br> </em>
                [<a href="https://arxiv.org/pdf/2106.02734.pdf">paper</a>] [<a
                  href="https://github.com/neu-spiral/HBaR">code</a>]
                <br>
                <font color=red> Invited oral presentation at <a
                    href="https://meetings.informs.org/wordpress/indianapolis2022/">INFORMS 2022</a></font> <br>
                <p></p>
                <p> We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck as a
                  regularizer for learning an adversarially robust deep neural network classifier, both theoretically
                  and empirically.</p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/smoking.png" data-id="smoking-carousel"></img>
                <div id="smoking-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/smoking.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="smoking-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009433">
                  <papertitle>Improved prediction of smoking status via isoform-aware RNA-seq deep learning models
                  </papertitle>
                </a>
                <br>
                <b>Zifeng Wang</b>, Aria Masoomi, Zhonghui Xu, Adel Boueiz, Sool Lee, Tingting Zhao, Russell Bowler,
                Michael Cho, Edwin K Silverman, Craig Hersh, Jennifer Dy, Peter J Castaldi<br>
                <em>PLOS Computational Biology, 2021. <br> </em>
                [<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009433">paper</a>]
                <br>
                <p></p>
                <p> We propose a novel deep learning model to incorporate prior knowledge regarding the relationship of
                  exons to transcript isoforms. We hypothesized that since smoking alters patterns of exon and isoform
                  usage, greater predictive accuracy could be obtained by using exon and isoform-level quantifications
                  to predict smoking status.</p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/lps.png" data-id="lps-carousel"></img>
                <div id="lps-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/lps.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="lps-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9338408">
                  <papertitle>Learn-Prune-Share for Lifelong Learning</papertitle>
                </a>
                <br>
                <b>Zifeng Wang*</b>, Tong Jian*, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, Stratis Ioannidis <br>
                <em>International Conference on Data Mining (<b>ICDM</b>), 2020.<br> </em>
                [<a href="https://arxiv.org/pdf/2012.06956.pdf">paper</a>]
                <br>
                <p></p>
                <p> We propose a learn-prune-share (LPS) algorithm which addresses the challenges of catastrophic
                  forgetting, parsimony, and knowledge reuse simultaneously.</p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/knet.png" data-id="knet-carousel"></img>
                <div id="knet-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/knet.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="knet-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9338431">
                  <papertitle>Open-world class discovery with kernel networks</papertitle>
                </a>
                <br>
                <b>Zifeng Wang</b>, Batool Salehi, Andrey Gritsenko, Kaushik Chowdhury, Stratis Ioannidis, Jennifer Dy
                <br>
                <em>International Conference on Data Mining (<b>ICDM</b>), 2020.<br> </em>
                <font color=red> Best paper candidate </font> <br>
                [<a href="https://arxiv.org/pdf/2012.06957.pdf">paper</a>]
                <br>
                <p></p>
                <p> We propose Class Discovery Kernel Network with Expansion (CD-KNet-Exp), a deep learning framework
                  for open-world class discovery problem.</p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/ifg.png" data-id="ifg-carousel"></img>
                <div id="ifg-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/ifg.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="ifg-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://proceedings.neurips.cc/paper/2020/hash/9b10a919ddeb07e103dc05ff523afe38-Abstract.html">
                  <papertitle>Instance-wise Feature Grouping</papertitle>
                </a>
                <br>
                Aria Masoomi, Chieh Wu, Tingting Zhao, <b>Zifeng Wang</b>, Peter Castaldi, Jennifer Dy <br>
                <em>Neural Information Processing Systems (<b>NeurIPS</b>), 2020. <br> </em>
                [<a href="https://zhaottcrystal.github.io/pdf/InstanceFeatureSelection.pdf">paper</a>]
                <br>
                <p></p>
                <p> We formally define two types of redundancies using information theory: Representation and Relevant
                  redundancies. We leverage these redundancies to design a formulation for instance-wise feature group
                  discovery and reveal a theoretical guideline to help discover the appropriate number of groups.</p>
              </td>
            </tr>


            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/new_device.png" data-id="new_device-carousel"></img>
                <div id="new_device-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/new_device.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="new_device-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/8935862">
                  <papertitle>Finding a ‘new’ needle in the haystack: Unseen radio detection in large populations using
                    deep learning</papertitle>
                </a>
                <br>
                Andrey Gritsenko*, <b>Zifeng Wang*</b>, Tong Jian, Jennifer Dy, Kaushik Chowdhury, Stratis Ioannidis
                <br>
                <em> IEEE International Symposium on Dynamic Spectrum Access Networks (<b>DySPAN</b>), 2019. <br> </em>
                [<a
                  href="https://www.researchgate.net/profile/Andrey-Gritsenko/publication/338074668_Finding_a_'New'_Needle_in_the_Haystack_Unseen_Radio_Detection_in_Large_Populations_Using_Deep_Learning/links/5e5d512a4585152ce80104af/Finding-a-New-Needle-in-the-Haystack-Unseen-Radio-Detection-in-Large-Populations-Using-Deep-Learning.pdf">paper</a>]
                [<a
                  href="https://coe.northeastern.edu/news/ece-team-wins-best-paper-award-at-ieee-dyspan-2019/">news</a>]
                <br>
                <font color=red> Best paper award </font> <br>
                <p></p>
                <p> We propose a novel approach that facilitates new class detection without retraining a neural
                  network, and perform extensive analysis of the proposed model both in terms of model parameters and
                  real-world datasets.</p>
              </td>
            </tr>

            <tr>
              <td width="32%">
                <img class="thumbnail" src="paper_images/cdrl.png" data-id="cdrl-carousel"></img>
                <div id="cdrl-carousel" class="carousel">
                  <img class="img-center-carousel" src="paper_images/cdrl.png" alt="">
                  <a href="javascript:void(0)"><img src="images/close-transparent.png" alt=""
                      data-carousel-id="cdrl-carousel" class="carousel-close"></a>
                </div>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a
                  href="https://openaccess.thecvf.com/content_ECCV_2018/html/Liangliang_Ren_Collaborative_Deep_Reinforcement_ECCV_2018_paper.html">
                  <papertitle>Collaborative deep reinforcement learning for multi-object tracking</papertitle>
                </a>
                <br>
                Liangliang Ren, Jiwen Lu, <b>Zifeng Wang</b>, Qi Tian, Jie Zhou <br>
                <em> European Conference on Computer Vision (<b>ECCV</b>), 2018. <br> </em>
                [<a
                  href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Liangliang_Ren_Collaborative_Deep_Reinforcement_ECCV_2018_paper.pdf">paper</a>]
                <br>
                <p></p>
                <p> We propose a collaborative deep reinforcement learning (C-DRL) method for multi-object tracking.
                  Specifically, we consider each object as an agent and track it via the prediction network, and seek
                  the optimal tracked results by exploiting the collaborative interactions of different agents and
                  environments via the decision network</p>
              </td>
            </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Invited Talks</heading>
                  <ul>
                    <li>
                      <papertitle>Sparse Continual Learning on the Edge</papertitle><br />
                      <em>- <a href="https://www.continualai.org/">ContinualAI</a>, 2023, Remote <br />
                        <em>- <a href="http://www.aitime.cn/">AI Time</a>, 2023, Remote (Chinese
                          version) </em>
                    <li>
                      <papertitle>Prompting-based Continual Learning</papertitle><br />
                      <em>- <a href="https://theaitalks.org/">AI Talks SG</a>, 2022, Remote </em><br />
                      <em>- <a href="https://www.continualai.org/">ContinualAI</a>, 2022, Remote </em>[<a
                        href="https://www.youtube.com/watch?v=19bylhGhfAw&list=PLm6QXeaB-XkBMFxvgZvYjqhaPgGg8Um9Z&index=1">recording</a>]
                    <li>
                      <papertitle>Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness
                      </papertitle><br />
                      <em>- <a href="https://meetings.informs.org/wordpress/indianapolis2022/">INFORMS Annual
                          Meeting</a>,
                        2022, Indianapolis, Indiana</em> [<a
                        href="https://www.abstractsonline.com/pp8/#!/10693/presentation/4612">abstract</a>]<br />
                      <em>- <a href="http://www.aitime.cn/">AI Time</a>, 2022, Remote (Chinese
                        version) </em>
                      [<a href="https://www.koushare.com/video/videodetail/24409">recording</a>][<a
                        href="http://www.aitime.cn/Activity/activityDetail?id=359">blog</a>]


                  </ul>
                </td>
              </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Awards</heading>
                  <ul>
                    <li><text style="color:black">Outstanding Student Award in Research</text>, Northeastern University,
                      2023 <br />
                    <li><text style="color:black">Scholar Award</text>, NeurIPS 2022 <br />
                    <li><text style="color:black">Best Paper Candidate</text>, ICDM 2020 <br />
                    <li><text style="color:black">Best Paper Award</text>, DySPAN 2019 <br />
                    <li><text style="color:black">Travel Award</text>, DySPAN 2019 <br />
                    <li><text style="color:black">Travel Award</text>, NeurIPS 2019 <br />
                    <li><text style="color:black">Dean's Fellowship</text>, Northeastern University, 2018 <br />
                      <font color="grey" style="italic" size="2pt"><em>&nbsp&nbsp Highest honor awarded to new PhD
                          students for out standing academic background.</em></font>
                    <li><text style="color:black">Evergrande Scholarship</text>, Tsinghua University, 2016 <br />
                      <font color="grey" style="italic" size="2pt"><em>&nbsp&nbsp Awarded to students with excellent
                          academic performance, scientific potential and overall development.</em> </font>
                  </ul>
                </td>
              </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Academic Service</heading>
                  <p>
                    <strong>Conference Reviewer</strong>: NeurIPS 21-23, ICML 21-23, ICLR 22-23, CVPR 22-23, ICCV 23,
                    ACL ARR <br />
                    <strong>PC Member</strong>: SDM 23 <br />
                    <strong>Journal Reviewer</strong>: TPAMI, TMLR, Neural Networks
                  </p>
                </td>
              </tr>
            </table>



            <div align="center">
              <div style="height:200px; width:200px;">
                <script type="text/javascript" id="clstr_globe"
                  src="//clustrmaps.com/globe.js?d=PtmqIDsoG0APo3esKNzTVAOJtnkg9IGD6POJuZBuXCk"></script>
              </div>
            </div>

            <p align="right">
              Template Credit: <a href="https://jonbarron.info/">Jon Barron</a>
            </p>

          </table> <!--  Intro + Publications -->



        </td>
      </tr>
  </table>
  <!--main text-->

</body>

</html>